---
title: "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-04-02 19:00:00 +0900
# categories: [AI | 딥러닝, Architecture]
# categories: [AI | 딥러닝, Concept]
categories: [AI | 딥러닝, Paper]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
# categories: [STEM | 수학/통계, Statistics]
tags: [DeepLearning, RoBERTa, BERT]
description: "RoBERTa 논문을 꼼꼼히 살펴봅시다."
image: assets/img/posts/resize/output/roberta-eval.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

<div align="center">
  <small>Source: <a href="https://arxiv.org/pdf/1907.11692">https://arxiv.org/pdf/1907.11692</a></small>
</div>

## RoBERTa

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692) 내용을 자세히 살펴봅니다.

### Abstract

**[원문]**

"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different
sizes, and, as we will show, hyperparameter
choices have significant impact on the final results. We present a replication study of BERT
pretraining (Devlin et al.
, 2019) that carefully
measures the impact of many key hyperparameters and training data size. We find that BERT
was significantly undertrained, and can match
or exceed the performance of every model
published after it. Our best model achieves
state-of-the-art results on GLUE, RACE and
SQuAD. These results highlight the importance of previously overlooked design choices,
and raise questions about the source of recently reported improvements. We release our
models and code."

저자들은 BERT를 Pretrain 하는데 있어서 undertrained 되었다고 언급하며 더욱 개선된 학습 방법으로 학습하여 GLUE, RACE, SQuAD 벤치마크에서 최고 성능을 달성한 연구 결과를 제시합니다.

### Introduction

"Self-training methods such as ELMo (Peters et al.
,
2018), GPT (Radford et al.
, 2018), BERT
(Devlin et al.
, 2019), XLM (Lample and Conneau
,
2019), and XLNet (Yang et al.
, 2019) have
brought significant performance gains, but it can
be challenging to determine which aspects of
the methods contribute the most. Training is
computationally expensive, limiting the amount
of tuning that can be done, and is often done with
private training data of varying sizes, limiting
our ability to measure the effects of the modeling
advances. "



