---
title: "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-04-02 19:00:00 +0900
# categories: [AI | 딥러닝, Architecture]
# categories: [AI | 딥러닝, Concept]
categories: [AI | 딥러닝, Paper]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
# categories: [STEM | 수학/통계, Statistics]
tags: [DeepLearning, RoBERTa, BERT]
description: "RoBERTa 논문을 꼼꼼히 살펴봅시다."
image: assets/img/posts/resize/output/roberta-eval.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

<div align="center">
  <small>Source: <a href="https://arxiv.org/pdf/1907.11692">https://arxiv.org/pdf/1907.11692</a></small>
</div>

## RoBERTa

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692) 내용을 자세히 살펴보고 중요한 내용를 정리합니다.

### Abstract

**[원문]**

"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different
sizes, and, as we will show, hyperparameter
choices have significant impact on the final results. We present a replication study of BERT
pretraining (Devlin et al.
, 2019) that carefully
measures the impact of many key hyperparameters and training data size. We find that BERT
was significantly undertrained, and can match
or exceed the performance of every model
published after it. Our best model achieves
state-of-the-art results on GLUE, RACE and
SQuAD. These results highlight the importance of previously overlooked design choices,
and raise questions about the source of recently reported improvements. We release our
models and code."

저자들은 BERT를 Pretrain 하는데 있어서 undertrained 되었다고 언급하며 더욱 개선된 학습 방법으로 학습하여 GLUE, RACE, SQuAD 벤치마크에서 최고 성능을 달성한 연구 결과를 제시합니다.

### 1. Introduction

"Self-training methods such as ELMo (Peters et al.
,
2018), GPT (Radford et al.
, 2018), BERT
(Devlin et al.
, 2019), XLM (Lample and Conneau
,
2019), and XLNet (Yang et al.
, 2019) have
brought significant performance gains, but it can
be challenging to determine which aspects of
the methods contribute the most. Training is
computationally expensive, limiting the amount
of tuning that can be done, and is often done with
private training data of varying sizes, limiting
our ability to measure the effects of the modeling
advances. "

과소학습된 기존 BERT에서 더욱 큰 batch-size/datasets, NSP(Next Sentence Prediction) 방식 제거, 더욱 긴 sequence 길이, dynamic masking pattern 학습 방식을 적용하여 성능을 끌어올린다고 언급합니다.  

"When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD.
When trained for longer over additional data, our
model achieves a score of 88.5 on the public
GLUE leaderboard, matching the 88.4 reported
by Yang et al.
(2019). Our model establishes a
new state-of-the-art on 4/9 of the GLUE tasks:
MNLI, QNLI, RTE and STS-B. We also match
state-of-the-art results on SQuAD and RACE.
Overall, we re-establish that BERT’s masked language model training objective is competitive
with other recently proposed training objectives
such as perturbed autoregressive language modeling (Yang et al., 2019).2"

동일한 학습 데이터 기준으로 BERT 대비 GLUE, SQuAD 벤치마크에서 성능 향상을 보였고, GLUE의 9개 task에서 MNLI, QNLI, RTE, STS-B task에서 새로운 sota 성능을 발휘하였다고 합니다.

### 2. Background

해당 section에서는 기존 BERT 모델에 대한 배경 설명 부분으로 아래와 같이 요약됩니다.

- 두 segment를 연결한 입력과 특수 토큰([CLS], [SEP], [EOS])을 사용
- transformer 기반 (L개 layer, A개 attention head, H 차원)
- MLM: 토큰의 15%를 마스킹하고 예측
- NSP: 두 segment가 연속적인지 예측하는 이진 분류
- Adam 사용
- 학습 데이터는 BOOKCORPUS와 영어 WIKIPEDIA (총 16GB)
- 1,000,000번 업데이트, batch-size 256, 최대 sequence 512 tokens

### 3. Experimental Setup

BERT와 유사한(논문에서느 replication이라고 표현) 연구를 위한 실험 설정에 대한 내용입니다.

#### 3.1 Implementation

- Adam $$\epsilon$$ 값이 성능에 큰 영향을 미치므로 큰 batch에서는 $$beta_{2} = 0.98$$로 설정
- 512 token 길이의 시퀀스만 학습

#### 3.2 Data

- 총 160GB 이상
- BOOKCORPUS(영어 WIKIPEDIA, 16GB), CC-NEWS(영어 뉴스 기사, 76GB), OPENWEBTEXT(Reddit, 38GB), STORIES(CommonCrawl 데이터의 하위 집합, 31GB)

#### 3.3 Evaluation

- 세 가지 벤치마크를 사용하여 사전학습된 모델을 downstream 작업에서 평가합니다.
- GLUE: 단일 문장 분류 또는 문장 쌍 분류 작업으로 구성
- SQuAD: 문맥에서 관련 스팬(span)을 추출하여 질문에 답하는 Stanford 질문 응답 데이터셋
- RACE: 중국의 영어 시험에서 수집된 8,000개 이상의 지문과 거의 100,000개의 독해 데이터셋, 네 가지 옵션 중에서 하나의 정답을 선택
  
### 4. Training Procedure Analysis

"This section explores and quantifies which choices
are important for successfully pretraining BERT
models. We keep the model architecture fixed.7
Specifically, we begin by training BERT models
with the same configuration as BERTBASE (L =
12, H = 768, A = 12, 110M params)."

기존 BERT BASE와 동일한 구성(L = 12, H = 768, A = 12, 110M 파라미터)으로 BERT 모델 학습합니다.

#### 4.1 Static vs. Dynamic Masking

"As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking
once during data preprocessing, resulting in a single static mask. To avoid using the same mask for
each training instance in every epoch, training data
was duplicated 10 times so that each sequence is
masked in 10 different ways over the 40 epochs of
training. Thus, each training sequence was seen
with the same mask four times during training.
We compare this strategy with dynamic masking where we generate the masking pattern every
time we feed a sequence to the model. This becomes crucial when pretraining for more steps or
with larger datasets.
"

기존 BERT는 masking을 한 번 수행하여 static masking를 생성했습니다. 매 epoch마다 동일한 mask를 사용하는 것을 피하기 위해, 10번 복제되어 40 epoch의 학습 기간 동안 각 sequence가 10가지 다른 방식으로 마스킹되도록 했습니다.

![image](https://github.com/user-attachments/assets/9bd97f88-7e84-412a-9808-83d2a3a856bb)

"Results Table 1 compares the published
BERTBASE results from Devlin et al. (2019) to our
reimplementation with either static or dynamic
masking. We find that our reimplementation
with static masking performs similar to the
original BERT model, and dynamic masking is
comparable or slightly better than static masking.
Given these results and the additional efficiency
benefits of dynamic masking, we use dynamic
masking in the remainder of the experiments."

dynamic masking은 static masking과 비슷하거나 약간 더 나은 성능을 보입니다. 저자들은 이후 실험에서도 dynamic masking을 사용합니다.

#### 4.2 Model Input Format and Next Sentence Prediction

"In the original BERT pretraining procedure, the
model observes two concatenated document segments, which are either sampled contiguously
from the same document (with p = 0.5) or from
distinct documents. In addition to the masked language modeling objective, the model is trained to
predict whether the observed document segments
come from the same or distinct documents via an
auxiliary Next Sentence Prediction (NSP) loss.
The NSP loss was hypothesized to be an important factor in training the original BERT model.
Devlin et al. (2019) observe that removing NSP
hurts performance, with significant performance
degradation on QNLI, MNLI, and SQuAD 1.1.
However, some recent work has questioned the
necessity of the NSP loss (Lample and Conneau,
2019; Yang et al., 2019; Joshi et al., 2019).
To better understand this discrepancy, we compare several alternative training formats:"

Devlin et al.(2019) 연구에 의하면 NSP를 제거했을 때 QNLI, MNLI 및 SQuAD 1.1에서 성능 저하가 있다고 관찰했습니다. 하지만 본 논문에서는 이에 대한 다른 대안을 제시합니다.

![image](https://github.com/user-attachments/assets/3c391556-8c15-4680-bb37-dff087ad84ca)

위 Table 2에서 

- SEGMENT-PAIR+NSP: 기존 BERT 입력 형식, 각 입력은 segment 쌍, 각 segment는 여러 문장 포함, 총 결합 길이는 512 토큰 
- SENTENCE-PAIR+NSP: 각 입력은 한 문서의 연속적인 부분에서 샘플링 or 별도의 문서에서 샘플링된 자연 문장 쌍
- FULL-SENTENCES: 각 입력은 하나 이상의 문서에서 연속적으로 샘플링된 전체 문장, 최대 512 token , NSP 제거, 문서 사이에 sep token
- DOC-SENTENCES: 입력은 FULL-SENTENCES와 유사, 문서 경계 overlap X

  
