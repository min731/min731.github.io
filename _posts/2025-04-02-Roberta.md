---
title: "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-04-02 19:00:00 +0900
# categories: [AI | 딥러닝, Architecture]
# categories: [AI | 딥러닝, Concept]
categories: [AI | 딥러닝, Paper]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
# categories: [STEM | 수학/통계, Statistics]
tags: [DeepLearning, RoBERTa, BERT]
description: "RoBERTa 논문을 꼼꼼히 살펴봅시다."
image: assets/img/posts/resize/output/roberta-eval.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

<div align="center">
  <small>Source: <a href="https://arxiv.org/pdf/1907.11692">https://arxiv.org/pdf/1907.11692</a></small>
</div>

## RoBERTa

[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692) 내용을 자세히 살펴보고 중요한 내용를 정리합니다.

### Abstract

**[원문]**

"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different
sizes, and, as we will show, hyperparameter
choices have significant impact on the final results. We present a replication study of BERT
pretraining (Devlin et al.
, 2019) that carefully
measures the impact of many key hyperparameters and training data size. We find that BERT
was significantly undertrained, and can match
or exceed the performance of every model
published after it. Our best model achieves
state-of-the-art results on GLUE, RACE and
SQuAD. These results highlight the importance of previously overlooked design choices,
and raise questions about the source of recently reported improvements. We release our
models and code."

저자들은 BERT를 Pretrain 하는데 있어서 undertrained 되었다고 언급하며 더욱 개선된 학습 방법으로 학습하여 GLUE, RACE, SQuAD 벤치마크에서 최고 성능을 달성한 연구 결과를 제시합니다.

### 1. Introduction

"Self-training methods such as ELMo (Peters et al.
,
2018), GPT (Radford et al.
, 2018), BERT
(Devlin et al.
, 2019), XLM (Lample and Conneau
,
2019), and XLNet (Yang et al.
, 2019) have
brought significant performance gains, but it can
be challenging to determine which aspects of
the methods contribute the most. Training is
computationally expensive, limiting the amount
of tuning that can be done, and is often done with
private training data of varying sizes, limiting
our ability to measure the effects of the modeling
advances. "

과소학습된 기존 BERT에서 더욱 큰 batch-size/datasets, NSP(Next Sentence Prediction) 방식 제거, 더욱 긴 sequence 길이, dynamic masking pattern 학습 방식을 적용하여 성능을 끌어올린다고 언급합니다.  

"When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD.
When trained for longer over additional data, our
model achieves a score of 88.5 on the public
GLUE leaderboard, matching the 88.4 reported
by Yang et al.
(2019). Our model establishes a
new state-of-the-art on 4/9 of the GLUE tasks:
MNLI, QNLI, RTE and STS-B. We also match
state-of-the-art results on SQuAD and RACE.
Overall, we re-establish that BERT’s masked language model training objective is competitive
with other recently proposed training objectives
such as perturbed autoregressive language modeling (Yang et al., 2019).2"

동일한 학습 데이터 기준으로 BERT 대비 GLUE, SQuAD 벤치마크에서 성능 향상을 보였고, GLUE의 9개 task에서 MNLI, QNLI, RTE, STS-B task에서 새로운 sota 성능을 발휘하였다고 합니다.

### 2. Background

해당 section에서는 기존 BERT 모델에 대한 배경 설명 부분으로 아래와 같이 요약됩니다.

- 두 segment를 연결한 입력과 특수 토큰([CLS], [SEP], [EOS])을 사용
- transformer 기반 (L개 layer, A개 attention head, H 차원)
- MLM: 토큰의 15%를 마스킹하고 예측
- NSP: 두 segment가 연속적인지 예측하는 이진 분류
- Adam 사용
- 학습 데이터는 BOOKCORPUS와 영어 WIKIPEDIA (총 16GB)
- 1,000,000번 업데이트, batch-size 256, 최대 sequence 512 tokens

(작성중)
