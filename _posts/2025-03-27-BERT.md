---
title: "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-03-27 19:00:00 +0900
# categories: [AI | 딥러닝, Architecture]
# categories: [AI | 딥러닝, Concept]
categories: [AI | 딥러닝, Paper]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
# categories: [STEM | 수학/통계, Statistics]
tags: [DeepLearning, BERT, Bidirectional Transformers]
description: "BERT 논문을 살펴보고 주요 특징들을 정리해봅시다."
image: assets/img/posts/resize/output/BERT_input_embeddings.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

<div align="center">
  <small>Source: <a href="https://commons.wikimedia.org/wiki/File:BERT_input_embeddings.png">https://commons.wikimedia.org/wiki/File:BERT_input_embeddings.png</a></small>
</div>

(작성중...)