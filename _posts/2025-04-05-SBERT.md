---
title: "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-04-05 12:30:00 +0900
# categories: [AI | 딥러닝, Architecture]
# categories: [AI | 딥러닝, Concept]
categories: [AI | 딥러닝, Paper]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
# categories: [STEM | 수학/통계, Statistics]
tags: [DeepLearning, Sentence-BERT, SBERT, BERT]
description: "Sentence-BERT 논문을 읽고 핵심 내용을 짚어봅니다."
image: assets/img/posts/resize/output/SBERT-architecture.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

<div align="center">
  <small>Source: <a href="https://arxiv.org/pdf/1908.10084">https://arxiv.org/pdf/1908.10084</a></small>
</div>

## Sentence-BERT

[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084)을 자세히 살펴보고 중요한 내용를 정리합니다.

### Abstract

"BERT (Devlin et al., 2018) and RoBERTa (Liu
et al., 2019) has set a new state-of-the-art
performance on sentence-pair regression tasks
like semantic textual similarity (STS). However, it requires that both sentences are fed
into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences
requires about 50 million inference computations (~65 hours) with BERT. The construction
of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks
like clustering."

저자들은 BERT, RoBERTa 아키텍처들이 문장과 문장간의 유사성을 구하는 STS task에서 sota 성능을 달성했지만, 이 모델들은 두 문장을 한번에 네트워크에 입력하기 때문에 계산하는데 큰 overhead를 발생시킨다고 언급하고 있습니다. 또한 문장간의 유사성을 구하거나 비지도 학습 기반 task를 수행하는데 적합하지 않다고 주장합니다.

"In this publication, we present Sentence-BERT
(SBERT), a modification of the pretrained
BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the
effort for finding the most similar pair from 65
hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.
We evaluate SBERT and SRoBERTa on common (전이학습 + STS 데이터셋 기준) STS tasks and transfer learning tasks,
where it outperforms other state-of-th

저자들은 이를 개선하기 위한 'Sentence-BERT(SBERT)'를 제안합니다. 'siamese' 및 'triplet' 구조를 사용하여 문장 임베딩을 표현한다고 합니다. 해당 방법을 통해 유사한 문장을 찾는 시간을 (전이학습 + STS 데이터셋 기준) BERT/RoBERTa의 65시간에서 SBERT의 5초로 최적화하면서 BERT의 정확도는 유지한다고 언급합니다.

### 1. Introduction

"BERT set new state-of-the-art performance on
various sentence classification and sentence-pair
regression tasks. BERT uses a cross-encoder: Two
sentences are passed to the transformer network
and the target value is predicted. However, this
setup is unsuitable for various pair regression tasks
due to too many possible combinations. Finding
in a collection of n = 10 000 sentences the pair
with the highest similarity requires with BERT
n·(n−1)/2 = 49 995 000 inference computations.
On a modern V100 GPU, this requires about 65
hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar
for a new question could be modeled as a pair-wise
comparison with BERT, however, answering a single query would require over 50 hours."

BERT는 두 문장을 하나의 입력으로 받는 encoder 구조인 'cross-encoder'로, 너무 많은 조합을 생성하기 때문에 적합하지 않을 수 있다고 언급합니다. $$n=10000$$ 라면 그 combination인 $$n(n-1)/2=49995000$$번의 계산이 필요하기 때문에 비효율적이라고 말합니다.

"To alleviate this issue, we developed SBERT.
The siamese network architecture enables that
fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosinesimilarity or Manhatten / Euclidean distance, semantically similar sentences can be found. These
similarity measures can be performed extremely
efficient on modern hardware, allowing SBERT
to be used for semantic similarity search as well
as for clustering. The complexity for finding the
arXiv:1908.10084v1 [cs.CL] 27 Aug 2019
most similar sentence pair in a collection of 10,000
sentences is reduced from 65 hours with BERT to
the computation of 10,000 sentence embeddings
(~5 seconds with SBERT) and computing cosinesimilarity (~0.01 seconds). By using optimized
index structures, finding the most similar Quora
question can be reduced from 50 hours to a few
milliseconds (Johnson et al., 2017)."

이를 해결하기 위해 'Siamese' 네트워크를 가진 SBERT를 통해서 입력 문장에 대한 크기를 고정시키며 cosine similarity 나 Euclidean/Manhattan 거리를 통해 유사한 문장들을 찾을 수 있다고 합니다. 'Siamese' 네트워크를 통해 하드웨어 관점에서 효율적으로 수행될 수 있습니다.

"We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding
methods like InferSent (Conneau et al., 2017) and
Universal Sentence Encoder (Cer et al., 2018). On
seven Semantic Textual Similarity (STS) tasks,
SBERT achieves an improvement of 11.7 points
compared to InferSent and 5.5 points compared to
Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for
sentence embeddings, we achieve an improvement
of 2.1 and 2.6 points, respectively."

![](assets/img/posts/resize/output/sbert-table1.png){: width="1000px"}

SBERT는 NLI 데이터로 fine-tunning 하였다고 합니다. 위 표와 같이 7개 STS task에서 InferSent 대비 11.7, Universal Sentence Encoder 대비 5.5 향상을 이루었습니다. 또 문장 임베딩을 평가하는 SentEval에서 각각 2.6 , 2.6 향상을 거두었다고 합니다.

### 2. Related Work

"A large disadvantage of the BERT network
structure is that no independent sentence embed-
dings are computed, which makes it difficult to de-
rive sentence embeddings from BERT. To bypass
this limitations, researchers passed single sen-
tences through BERT and then derive a fixed sized
vector by either averaging the outputs (similar to
average word embeddings) or by using the output
of the special CLS token (for example: May et al.
(2019); Zhang et al. (2019); Qiao et al. (2019)).
These two options are also provided by the popu-
lar bert-as-a-service-repository3 . Up to our knowl-
edge, there is so far no evaluation if these methods
lead to useful sentence embeddings."

이전 연구에 대한 내용입니다.

BERT 네트워크의 큰 단점인 단일한 문장을 임베딩한다는 점을 개선하기 위해, 단일한 문장을 BERT에 통과시킨 뒤 출력을 평균화 하는 방식(평균 단어 임베딩 방식)이나
CLS 토큰을 고정 크기 벡터 출력하는 방식이 제안하기도 하였습니다.

"Previous neural sentence embedding methods
started the training from a random initialization.
In this publication, we use the pre-trained BERT
and RoBERTa network and only fine-tune it to
yield useful sentence embeddings. This reduces
significantly the needed training time: SBERT can
be tuned in less than 20 minutes, while yielding
better results than comparable sentence embed-
ding methods."

이전 다양한 문장 임베딩 연구에서는 random initial weight를 사용하였습니다. 저자들은 본 논문에서 pretrained 된 BERT나 RoBERTa 기반으로
이를 fine-tunning 하는 방식으로 진행합니다. 이를 통해 전체 Training 시간을 줄여 SBERT를 20분 이내에 학습시키고
다른 문장 임베딩 방법들과 비슷한 성능을 도출한다고 합니다.

### 3. Model

"SBERT adds a pooling operation to the output
of BERT / RoBERTa to derive a fixed sized sen-
tence embedding. We experiment with three pool-
ing strategies: Using the output of the CLS-token,
computing the mean of all output vectors (MEAN-
strategy), and computing a max-over-time of the
output vectors (MAX-strategy). The default config-
uration is MEAN."

앞서 언급된 것처럼 SBERT는 BERT나 RoBERTa 네트워크 기반에 Pooling 연산을 추가하여 임베딩합니다.
본 연구에서는 CLS 토큰의 출력, 모든 출력 벡터의 평균, 출력 벡터들 사이에서의 최대값을 활용하는 등 3가지 방식을 실험합니다.
기본적으로는 모든 출력 벡터의 평균 방식인 'MEAN-strategy'로 진행됩니다.

"In order to fine-tune BERT / RoBERTa, we cre-
ate siamese and triplet networks (Schroff et al.,
2015) to update the weights such that the produced
sentence embeddings are semantically meaningful
and can be compared with cosine-similarity."

![](assets/img/posts/resize/output/sbert-table1.png){: width="1000px"}

SBERT의 핵심 요소로 'siamese', 'triplet nerworks' 활용한다는 내용입니다.

"The network structure depends on the availabletraining data. We experiment with the following
structures and objective functions."

원하는 task에 따라 구제척인 네트워크 구조와 objective function이 달라집니다. 

"Classification Objective Function. We con-
catenate the sentence embeddings u and v with
the element-wise difference |u − v| and multiply it
with the trainable weight Wt ∈ R3n×k :
o = softmax(Wt (u, v, |u − v|))
where n is the dimension of the sentence em-
beddings and k the number of labels. We optimize
cross-entropy loss. This structure is depicted in
Figure 1."

분류가 목적이라면, 문장을 임베딩 한 값 $$u,\ v$$와
임베딩 값의 차이 $$|u-v|$$를 concat하여 사용합니다.

$$o = softmax(W_t(u, v, |u-v|))$$

"Regression Objective Function. The cosine-
similarity between the two sentence embeddings
u and v is computed (Figure 2). We use mean-
squared-error loss as the objective function."

코사인 유사도를 활용하여 두 문장 유사도를 구한다면, 두 문장 임베딩 $$u,\ v$$를 활용하고 MSE를 손실함수로 사용합니다,

"Triplet Objective Function. Given an anchor
sentence a, a positive sentence p, and a negative
sentence n, triplet loss tunes the network such that
the distance between a and p is smaller than the
distance between a and n. Mathematically, we
minimize the following loss function:
$$max(||s_{a} - s_{p} || - ||s_{a} - s_{n} || + \epsilon, 0)$$
with s_{x} the sentence embedding for a/n/p, || · ||
a distance metric and margin $$\epsilon$$. Margin $$\epsilon$$ ensures
that $$s_{p}$$ is at least $$\epsilon$$ closer to sa than $$s_{n}$$ . As metric
we use Euclidean distance and we set $$\epsilon = 1$$ in our
experiments."

유클리드 거리 방식으로 측정한다면, 'Triplet Obejective Function'을 활용하게 됩니다.
이때 anchor sentence를 a, positive sentence를 p, negative sentence를 n으로 하며 a,p의 거리를
a,n으로의 거리보다 작도록 학습하면 됩니다. 

$$max(||s_{a} - s_{p} || - ||s_{a} - s_{n} || + \epsilon, 0)$$

$$s_{x}$$는  a/n/p에 대한 문장 임베딩 값이고, margin으로 표현한 $$\epsilon$$은 $$s_{p}$$가 $$\epsilon$$ 만큼은 $$s_{a}$$에
가깝게 보완하는 역할을 합니다.

#### 3.1 Training Details

"We train SBERT on the combination of the SNLI
(Bowman et al., 2015) and the Multi-Genre NLI(Williams et al., 2018) dataset. The SNLI is a col-
lection of 570,000 sentence pairs annotated with
the labels contradiction, eintailment, and neu-
tral. MultiNLI contains 430,000 sentence pairs
and covers a range of genres of spoken and written
text. We fine-tune SBERT with a 3-way softmax-
classifier objective function for one epoch. We
used a batch-size of 16, Adam optimizer with
learning rate 2e−5, and a linear learning rate
warm-up over 10% of the training data. Our de-
fault pooling strategy is MEAN"

contradiction, entailment, neutral 레이블로 이루어진 SNLI 데이터셋과 다양한 장르의 문장 쌍 데이터인 MultiNLI를 통해 
학습했다고 합니다. softmax-
classifier objective을 objective function으로 사용하였고 1 epoch, 16 batch-size, adam optimizer 2e-5, linear warm-up으로 
fine-tunning하였으며 pooling은 앞서 언급된 'MEAN-strategy'입니다.

### 4 Evaluation - Semantic Textual Similarity

#### 4.1 Unsupervised STS

"The results shows that directly using the output of BERT leads to rather poor performances. Averaging the BERT embeddings achieves an average correlation of only 54.81, and using the CLStoken output only achieves an average correlation of 29.19. Both are worse than computing average GloVe embeddings."

![](assets/img/posts/resize/output/sbert-table1.png){: width="1000px"}

Table 1에서 제시한 것과 같이 BERT의 임베딩 벡터를 평균화하는 방식은 54.81이며, CLS 토큰을 활용하는 방식은 29.19 였습니다.
이 두 방식 모두 Glove의 평균 임베딩 보다 성능이 떨어집니다.

"Using the described siamese network structure
and fine-tuning mechanism substantially improves
the correlation, outperforming both InferSent and
Universal Sentence Encoder substantially. The
only dataset where SBERT performs worse than
Universal Sentence Encoder is SICK-R. Universal
Sentence Encoder was trained on various datasets,
including news, question-answer pages and discussion forums, which appears to be more suitable
to the data of SICK-R. In contrast, SBERT was
pre-trained only on Wikipedia (via BERT) and on
NLI data."

"While RoBERTa was able to improve the performance for several supervised tasks, we only
observe minor difference between SBERT and
SRoBERTa for generating sentence embeddings."

SBERT가 Universal Sentence Encoder보다 성능이 떨어지는 유일한 데이터셋은 SICK-R입니다. RoBERTa가 여러 지도 학습 태스크에서 성능을 향상시킬 수 있었지만, 문장 임베딩 생성에 있어서는 SBERT와 SRoBERTa 간에 미미한 차이만 있습니다.

#### 4.2 Supervised STS

![image](https://github.com/user-attachments/assets/df58842e-6098-4e43-9332-7ffe32177e8c)

"The STS benchmark (STSb) (Cer et al., 2017) provides is a popular dataset to evaluate supervised
STS systems. The data includes 8,628 sentence
pairs from the three categories captions, news, and
forums. It is divided into train (5,749), dev (1,500)
and test (1,379)."

STS benchmark는 지도 학습 STS을 평가하기 위한 데이터셋이며, 이 데이터는 caption, 뉴스, forum의 세 가지 카테고리에서 가져온 8,628개의 문장 쌍을 포함합니다.
이는 train 세트(5,749), dev 세트(1,500), 그리고 test 세트(1,379)로 나뉩니다.


(작성중...)
