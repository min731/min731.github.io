---
title: "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-04-05 12:30:00 +0900
# categories: [AI | 딥러닝, Architecture]
# categories: [AI | 딥러닝, Concept]
categories: [AI | 딥러닝, Paper]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
# categories: [STEM | 수학/통계, Statistics]
tags: [DeepLearning, Sentence-BERT, SBERT, BERT]
description: "Sentence-BERT 논문을 읽고 핵심 내용을 짚어봅니다."
image: assets/img/posts/resize/output/SBERT-architecture.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

<div align="center">
  <small>Source: <a href="https://arxiv.org/pdf/1908.10084">https://arxiv.org/pdf/1908.10084</a></small>
</div>

## Sentence-BERT

[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084)을 자세히 살펴보고 중요한 내용를 정리합니다.

### Abstract

"BERT (Devlin et al., 2018) and RoBERTa (Liu
et al., 2019) has set a new state-of-the-art
performance on sentence-pair regression tasks
like semantic textual similarity (STS). However, it requires that both sentences are fed
into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences
requires about 50 million inference computations (~65 hours) with BERT. The construction
of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks
like clustering."

저자들은 BERT, RoBERTa 아키텍처들이 문장과 문장간의 유사성을 구하는 STS task에서 sota 성능을 달성했지만, 이 모델들은 두 문장을 한번에 네트워크에 입력하기 때문에 계산하는데 큰 overhead를 발생시킨다고 언급하고 있습니다. 또한 문장간의 유사성을 구하거나 비지도 학습 기반 task를 수행하는데 적합하지 않다고 주장합니다.

"In this publication, we present Sentence-BERT
(SBERT), a modification of the pretrained
BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the
effort for finding the most similar pair from 65
hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.
We evaluate SBERT and SRoBERTa on common (전이학습 + STS 데이터셋 기준) STS tasks and transfer learning tasks,
where it outperforms other state-of-th

저자들은 이를 개선하기 위한 'Sentence-BERT(SBERT)'를 제안합니다. 'siamese' 및 'triplet' 구조를 사용하여 문장 임베딩을 표현한다고 합니다. 해당 방법을 통해 유사한 문장을 찾는 시간을 (전이학습 + STS 데이터셋 기준) BERT/RoBERTa의 65시간에서 SBERT의 5초로 최적화하면서 BERT의 정확도는 유지한다고 언급합니다.

### 1. Introduction

"BERT set new state-of-the-art performance on
various sentence classification and sentence-pair
regression tasks. BERT uses a cross-encoder: Two
sentences are passed to the transformer network
and the target value is predicted. However, this
setup is unsuitable for various pair regression tasks
due to too many possible combinations. Finding
in a collection of n = 10 000 sentences the pair
with the highest similarity requires with BERT
n·(n−1)/2 = 49 995 000 inference computations.
On a modern V100 GPU, this requires about 65
hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar
for a new question could be modeled as a pair-wise
comparison with BERT, however, answering a single query would require over 50 hours."

BERT는 두 문장을 하나의 입력으로 받는 encoder 구조인 'cross-encoder'로, 너무 많은 조합을 생성하기 때문에 적합하지 않을 수 있다고 언급합니다. $$n=10000$$ 라면 그 combination인 $$n(n-1)/2=49995000$$번의 계산이 필요하기 때문에 비효율적이라고 말합니다.

"To alleviate this issue, we developed SBERT.
The siamese network architecture enables that
fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosinesimilarity or Manhatten / Euclidean distance, semantically similar sentences can be found. These
similarity measures can be performed extremely
efficient on modern hardware, allowing SBERT
to be used for semantic similarity search as well
as for clustering. The complexity for finding the
arXiv:1908.10084v1 [cs.CL] 27 Aug 2019
most similar sentence pair in a collection of 10,000
sentences is reduced from 65 hours with BERT to
the computation of 10,000 sentence embeddings
(~5 seconds with SBERT) and computing cosinesimilarity (~0.01 seconds). By using optimized
index structures, finding the most similar Quora
question can be reduced from 50 hours to a few
milliseconds (Johnson et al., 2017)."

이를 해결하기 위해 'Siamese' 네트워크를 가진 SBERT를 통해서 입력 문장에 대한 크기를 고정시키며 cosine similarity 나 Euclidean/Manhattan 거리를 통해 유사한 문장들을 찾을 수 있다고 합니다. 'Siamese' 네트워크를 통해 하드웨어 관점에서 효율적으로 수행될 수 있습니다.

"We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding
methods like InferSent (Conneau et al., 2017) and
Universal Sentence Encoder (Cer et al., 2018). On
seven Semantic Textual Similarity (STS) tasks,
SBERT achieves an improvement of 11.7 points
compared to InferSent and 5.5 points compared to
Universal Sentence Encoder. On SentEval (Conneau and Kiela, 2018), an evaluation toolkit for
sentence embeddings, we achieve an improvement
of 2.1 and 2.6 points, respectively."

![](assets/img/posts/resize/output/sbert-table1.png){: width="1000px"}

SBERT는 NLI 데이터로 fine-tunning 하였다고 합니다. 위 표와 같이 7개 STS task에서 InferSent 대비 11.7, Universal Sentence Encoder 대비 5.5 향상을 이루었습니다. 또 문장 임베딩을 평가하는 SentEval에서 각각 2.6 , 2.6 향상을 거두었다고 합니다.


(작성중...)

