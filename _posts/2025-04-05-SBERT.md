---
title: "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-04-05 12:30:00 +0900
# categories: [AI | 딥러닝, Architecture]
# categories: [AI | 딥러닝, Concept]
categories: [AI | 딥러닝, Paper]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
# categories: [STEM | 수학/통계, Statistics]
tags: [DeepLearning, Sentence-BERT, SBERT, BERT]
description: "Sentence-BERT 논문을 읽고 핵심 내용을 짚어봅니다."
image: assets/img/posts/resize/output/SBERT-architecture.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

<div align="center">
  <small>Source: <a href="https://arxiv.org/pdf/1908.10084">https://arxiv.org/pdf/1908.10084</a></small>
</div>

## Sentence-BERT

[Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084) 내용을 자세히 살펴봅니다.

### Abstract

"BERT (Devlin et al., 2018) and RoBERTa (Liu
et al., 2019) has set a new state-of-the-art
performance on sentence-pair regression tasks
like semantic textual similarity (STS). However, it requires that both sentences are fed
into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences
requires about 50 million inference computations (~65 hours) with BERT. The construction
of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks
like clustering."

저자들은 BERT, RoBERTa 아키텍처가 문장과 문장간의 유사성을 구하는 STS task에서 sota 성능을 달성했지만, 이 모델들은 두 문장을 한번에 네트워크에 입력하기 때문에 계산하는데 큰 overhead를 발생시킨다고 언급하고 있습니다.

### 1. Introduction

(작성중...)

