---
title: "LSTM"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-02-28 00:00:00 +0900
categories: [AI | 딥러닝 개념, Architecture]
# categories: [AI | 논문 리뷰, Attention is all you need]
# categories: [MLOps | 인프라 개발, Kserve]
# categories: [Life | 일상 이야기, 와플먹으면서 공부하기]
tags: [DeepLearning, LSTM]
description: "LSTM(Long Short-Term Memory) 구조를 알아봅시다."
image: /assets/img/posts/resize/output/06_LSTM.png # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

>  *본 게시글은 유튜브 ['김성범[ 교수 / 산업경영공학부 ]' [핵심 머신러닝 RNN, LSTM and GRU]](https://www.youtube.com/watch?v=006BjyZicCo&t=2588s) 자료를 참고한 점임을 알립니다.

## LSTM에 대해서

### 1. 기본 컨셉
- 장기 의존성 문제를 완화한 RNN 개선 모델
- 기존 RNN의 'Hidden State($$h_{t}$$)'와 더불어 'Cell State($$c_{t}$$)'를 제안
- Forget gate ($$f_{t}$$), Input gate ($$i_{t}$$), Output gate ($$o_{t}$$) 세가지 gate를 추가한 구조

![](https://velog.velcdn.com/images/min0731/post/b2b78db9-be17-4bda-bf66-deee8a6e48f9/image.png)

- $$h_{t}$$를 다른 방식으로 계산하여 장기 의존성 문제 완화
- 핵심인 $$h_{t}$$를 구하는 데 있어 $$c_{t}$$를 활용

### 2. 전체 구조

![](https://velog.velcdn.com/images/min0731/post/1bafbf67-d800-489f-8a98-824b518685ac/image.png)

- 최종 목표인 $$t$$시점의 $$y_{t}$$를 구하기 위한 $$h_{t}$$

### 3. 원리

 - 첫번째 단계 
 $$\tilde{c}_t$$(임시 Cell State)를 구하기 위한 $$x_{t}$$(현재 Input), $$h_{t-1}$$(이전 Hidden State)
![](https://velog.velcdn.com/images/min0731/post/7489c080-16a0-4bc2-8eff-ca4228efa412/image.png)

- 두번째 단계
$$c_{t}$$(현재 Cell State)를 구하기 위한 $$f_{t}$$(Forget gate), $$c_{t-1}$$(이전 Cell State), $$i_{t}$$(Input gate), $$\tilde{c}_t$$(임시 Cell State)

![](https://velog.velcdn.com/images/min0731/post/26f6c9cc-ff58-4818-9978-69801397de55/image.png)

- 세번째 단계
$$h_{t}$$(Hidden State)를 구하기 위한 $$o_{t}$$(Output gate), $$c_{t}$$(현재 Cell State)
![](https://velog.velcdn.com/images/min0731/post/5b9ea665-1925-436f-9403-ffb02d9b571c/image.png)

- 종합
![](https://velog.velcdn.com/images/min0731/post/1fb47835-2cd4-401d-b594-313c1e3af0a7/image.png)

### 4. Forget / Input / Output Gate
- 과거 Cell State에서 사용하지 않을 데이터에 대한 가중치
$$
f_{t}=\sigma(W_{xh_{f}}x_{t}+W_{hh_{f}}h_{t-1}+b_{h_{f}})
$$
![](https://velog.velcdn.com/images/min0731/post/594d3850-9879-4048-9b4e-4351cc24d039/image.png)

- 현재 Input에서 사용할 데이터를 저장하기 위한 가중치
$$
i_{t}=\sigma(W_{xh_{i}}x_{t}+W_{hh_{i}}h_{t-1}+b_{h_{i}})
$$
![](https://velog.velcdn.com/images/min0731/post/d1fdd941-e077-4b7d-8c47-783915f08e90/image.png)

- 출력할 Hidden State에 Cell State를 얼마나 반영할 것인지에 대한 가중치
$$
o_{t}=\sigma(W_{xh_{o}}x_{t}+W_{hh_{o}}h_{t-1}+b_{h_{o}})
$$
![](https://velog.velcdn.com/images/min0731/post/931e4969-c51d-4c40-aaec-044dc340e7ce/image.png)

- Gate는 0~1 사이의 값을 가지는 벡터

### 5. Cell State

- $$\tilde{c}_t$$ (임시 Cell State)<br>
$$
\tilde{c}_t = tanh(W_{xh_{g}}x_{t}+W_{hh_{g}}h_{t-1}+b_{h_{g}})
$$
![](https://velog.velcdn.com/images/min0731/post/5774c2a3-3104-4bf1-a393-07b6486aec38/image.png)

- $${c}_t$$ (현재 Cell State)<br>
$$
c_{t} = f_{t} \otimes c_{t-1} \oplus i_{t} \otimes \tilde{c}_t  (\otimes = elementwise-product)
$$
![](https://velog.velcdn.com/images/min0731/post/1bd26cca-9f14-4468-9eca-97f2935a165b/image.png)

- [1] Forget Gate의 역할 ($$c_{t-1}$$의 정보를 조절)
![](https://velog.velcdn.com/images/min0731/post/518eefdb-b2be-4e98-9b82-5247380dd2d0/image.png)

- [2] Input Gate의 역할 ($$\tilde c_{t}$$의 정보를 조절)
![](https://velog.velcdn.com/images/min0731/post/b06cab2a-6eae-49f6-9e6c-f34b79da046a/image.png)

- [1] + [2] 를 합하여 $${c}_t$$ (현재 Cell State)
![](https://velog.velcdn.com/images/min0731/post/bace957c-df8b-4df1-8bdf-14760c8e3831/image.png)

### 6. Hidden State

- Output Gate
$$
o_{t}=\sigma(W_{xh_{o}}x_{t}+W_{hh_{o}}h_{t-1}+b_{h_{o}})
$$

- Hidden State
$$
h_{t} = o_{t} \odot tanh(c_{t})
$$
![](https://velog.velcdn.com/images/min0731/post/ef8782d8-5e73-4cff-a6ee-543badf9812f/image.png)

### 7. ETC

- LSTM with peephole connections
- Gate에 $$c_{t}$$ or $$c_{t-1}$$ 정보를 활용

<br>

$$
f_{t}=\sigma(W_{xh_{f}}x_{t}+W_{hh_{f}}h_{t-1}+W_ch_{f}c_{t-1}+b_{h_{f}})
$$

$$
i_{t}=\sigma(W_{xh_{i}}x_{t}+W_{hh_{i}}h_{t-1}+W_ch_{i}c_{t-1}+b_{h_{i}})
$$

$$
o_{t}=\sigma(W_{xh_{o}}x_{t}+W_{hh_{o}}h_{t-1}+W_ch_{o}c_{t}+b_{h_{o}})
$$

![](https://velog.velcdn.com/images/min0731/post/0d950d28-56a0-4fbd-b061-51d8877d03a3/image.png)
- GRU
- LSTM의 구조를 개선하여 파라미터 개수를 줄임
- Forget Gate, Input Gate를 'Update Gate($$z_{t}$$)'로 통합
- Output Gate를 대체할 'Reset Gate($$r_{t}$$)' 정의
- Cell State, Hidden State를 'Hidden State'로 통합

![](https://velog.velcdn.com/images/min0731/post/05a3d8e9-62f8-42cf-a887-c54fb2db9fe5/image.png)

> 참고 자료
  
- ['김성범[ 교수 / 산업경영공학부 ]' [핵심 머신러닝 RNN, LSTM and GRU]](https://www.youtube.com/watch?v=006BjyZicCo&t=2588s)