---
title: "Entropy / Cross Entropy / KL Divergence"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-03-02 00:00:00 +0900
# categories: [STEM | ìˆ˜í•™/í†µê³„, Statistics]
categories: [AI | ë”¥ëŸ¬ë‹, Concept]
# categories: [AI ; ë…¼ë¬¸ ë¦¬ë·°, Attention is all you need]
# categories: [MLOps ; ì¸í”„ë¼ ê°œë°œ, Kserve]
# categories: [Life ; ì¼ìƒ ì´ì•¼ê¸°, ì™€í”Œë¨¹ìœ¼ë©´ì„œ ê³µë¶€í•˜ê¸°]
tags: [DeepLearning, Entropy, Cross Entropy, KL Divergence]
description: "Entropy, Cross Entropy, Kullbackâ€“Leibler Divergencì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤."
image: /assets/img/posts/resize/output/entropy.webp # ëŒ€í‘œ ì´ë¯¸ì§€  ê°€ë¡œ ì„¸ë¡œ ë¹„ìœ¨ ì•½ 1.91:1 (ì˜ˆ: 1200Ã—628px)
math: true
toc: true
# pin: true
---

**Photo by [https://www.v7labs.com/blog/cross-entropy-loss-guide](https://www.v7labs.com/blog/cross-entropy-loss-guide)**

> ## Entropy

### 1. ì •ë³´ ì´ë¡ ì—ì„œì˜ Entropy

- ì •ë³´ì˜ ë¶ˆí™•ì‹¤ì„±ì´ë‚˜ í˜¼ëˆ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œ
- ì •ë³´ ì´ë¡ ì—ì„œ ì—”íŠ¸ë¡œí”¼ëŠ” ì–´ë–¤ ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì— ë”°ë¼ ê·¸ ì‚¬ê±´ì˜ ì •ë³´ëŸ‰ì„ ì¸¡ì •í•©ë‹ˆë‹¤. ë°œìƒ í™•ë¥ ì´ ë‚®ì„ìˆ˜ë¡, ê·¸ ì‚¬ê±´ì´ ì£¼ëŠ” ì •ë³´ëŸ‰ì´ í½ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, ë°œìƒ í™•ë¥ ì´ ë†’ë‹¤ë©´ ê·¸ ì‚¬ê±´ì´ ì£¼ëŠ” ì •ë³´ëŸ‰ì€ ì ìŠµë‹ˆë‹¤.

- ì •ë³´ëŸ‰(Information Content)
$$
I(x) = -\log p(x)
$$

$$
(I(x):ì‚¬ê±´ xì˜ ì •ë³´ëŸ‰,\ p(x):ì‚¬ê±´ xê°€ ë°œìƒí•  í™•ë¥ )
$$

### 2. Shannon ì—”íŠ¸ë¡œí”¼ (Shannon Entropy)

$$
H(p) = - \sum_{i} p(x_i) \log p(x_i)
$$

$$
(H(p): í™•ë¥ \ ë¶„í¬\ pì˜\ ì—”íŠ¸ë¡œí”¼,\ p(x_i):ì‚¬ê±´ x_iê°€\ ë°œìƒí• \ í™•ë¥ )
$$

### 3. ê³µì •í•œ ë™ì „ ë˜ì§€ê¸°ì˜ ì—”íŠ¸ë¡œí”¼

$$
H = -[0.5 \log_2 0.5 + 0.5 \log_2 0.5] = 1
$$

- í•­ìƒ ì•ë©´ë§Œ ë‚˜ì˜¤ëŠ” ë™ì „ì˜ ì—”íŠ¸ë¡œí”¼

$$
H = -[1 \log_2 1 + 0 \log_2 0] = 0
$$

### 4. ì—°ì† í™•ë¥  ë¶„í¬ì—ì„œì˜ ì—”íŠ¸ë¡œí”¼ (Differential Entropy)

$$
H(X) = - \int_{-\infty}^{\infty} p(x) \log p(x) \, dx
$$

> ## Cross Entropy

### 1. Cross Entropy (êµì°¨ ì—”íŠ¸ë¡œí”¼)ë€?

êµì°¨ ì—”íŠ¸ë¡œí”¼(Cross Entropy)ëŠ” ì‹¤ì œ í™•ë¥  ë¶„í¬ $$p$$ì™€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  ë¶„í¬ $$q$$ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì£¼ë¡œ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¡œ ë§ì´ ì‚¬ìš©ë˜ë©°, ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  ë¶„í¬ê°€ ì‹¤ì œ ë¶„í¬ì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.

êµì°¨ ì—”íŠ¸ë¡œí”¼ëŠ” ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ í‰ê·  ì •ë³´ëŸ‰ì„ ë‚˜íƒ€ë‚´ë©°, ì •ë‹µ ë ˆì´ë¸”ê³¼ ì˜ˆì¸¡ ê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ëª©í‘œëŠ” ì˜ˆì¸¡ ë¶„í¬ $$q$$ë¥¼ ì‹¤ì œ ë¶„í¬
$$p$$ì™€ ê°€ê¹ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

### 2. êµì°¨ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì‹

êµì°¨ ì—”íŠ¸ë¡œí”¼ëŠ” ë‘ í™•ë¥  ë¶„í¬ $$ğ‘$$ì™€ $$q$$ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.

- ì¼ë°˜ì ì¸ êµì°¨ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì‹
$$
H(p, q) = - \sum_{i} p(x_i) \log q(x_i)
$$

$$
p(x_i): ì‹¤ì œ\ ë¶„í¬ì—ì„œ\ ì‚¬ê±´\ ğ‘¥_ğ‘–ê°€\ ë°œìƒí• \ í™•ë¥ \ (ground\ truth\ í™•ë¥ )
$$

$$
q(x_i): ì˜ˆì¸¡\ ë¶„í¬ì—ì„œ\ ì‚¬ê±´\ ğ‘¥_ğ‘–ê°€\ ë°œìƒí• \ í™•ë¥ \ (ëª¨ë¸ì´\ ì˜ˆì¸¡í•œ í™•ë¥ )
$$

ì´ ìˆ˜ì‹ì€ ê° ì‚¬ê±´ì— ëŒ€í•´ ì‹¤ì œ í™•ë¥ ê³¼ ì˜ˆì¸¡ í™•ë¥  ê°„ì˜ ì°¨ì´ë¥¼ ë¡œê·¸ë¡œ ê³„ì‚°í•œ í›„, ê·¸ ê°’ì„ ê°€ì¤‘í•©í•œ ê²ƒì…ë‹ˆë‹¤. êµì°¨ ì—”íŠ¸ë¡œí”¼ ê°’ì´ ì‘ì„ìˆ˜ë¡ ë‘ ë¶„í¬ ê°„ì˜ ì°¨ì´ê°€ ì‘ê³ , ê°’ì´ í´ìˆ˜ë¡ ì°¨ì´ê°€ í¬ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

- ì´ì§„ ë¶„ë¥˜ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ìˆ˜ì‹
$$
H(p, q) = -[p \log q + (1 - p) \log (1 - q)]
$$

$$
(p:\ ì‹¤ì œ\ ë ˆì´ë¸”\ (0 ë˜ëŠ” 1),\ q:\ ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ \  (0 ë˜ëŠ” 1))
$$

### 3. êµì°¨ ì—”íŠ¸ë¡œí”¼ì˜ íŠ¹ì§•

ê°’ì˜ í•´ì„: êµì°¨ ì—”íŠ¸ë¡œí”¼ ê°’ì´ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì´ ì‹¤ì œ ë¶„í¬ì— ê°€ê¹Œìš´ ì˜ˆì¸¡ì„ í•˜ê³  ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ê°’ì´ í´ìˆ˜ë¡ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì‹¤ì œì™€ ë§ì´ ë‹¤ë¥´ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

**ì •í™•í•œ ì˜ˆì¸¡ì¼ ë•Œ**: $$ğ‘(ğ‘¥_ğ‘–)$$ê°€ ì‹¤ì œ ê°’ê³¼ ë§¤ìš° ê°€ê¹Œìš°ë©´ $$logğ‘(ğ‘¥_ğ‘–)$$ê°€ ì»¤ì§€ê³ , êµì°¨ ì—”íŠ¸ë¡œí”¼ ê°’ì€ ì‘ì•„ì§‘ë‹ˆë‹¤.

**ì˜ëª»ëœ ì˜ˆì¸¡ì¼ ë•Œ**: $$ğ‘(ğ‘¥_ğ‘–)$$ê°€ ì‹¤ì œ ê°’ê³¼ ë§ì´ ë‹¤ë¥¼ìˆ˜ë¡ $$logğ‘(ğ‘¥_ğ‘–)$$ëŠ” ë§¤ìš° ì‘ì•„ì§€ê±°ë‚˜ ìŒìˆ˜ê°€ ë˜ì–´ êµì°¨ ì—”íŠ¸ë¡œí”¼ ê°’ì´ ì»¤ì§‘ë‹ˆë‹¤.

> ## KL Divergence

### 1. KL Divergenceë€?

KL Divergence (Kullback-Leibler Divergence)ëŠ” ë‘ í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ì‹¤ì œ ë¶„í¬ $$p$$ì™€ ì˜ˆì¸¡ ë¶„í¬ $$q$$ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. KL DivergenceëŠ” ë¹„ëŒ€ì¹­ì ì¸ ì²™ë„ì´ë©°, **ì •ë³´ ì†ì‹¤ ë˜ëŠ” ì¶”ì •ì˜ ë¹„íš¨ìœ¨ì„±ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‘œí˜„**í•©ë‹ˆë‹¤. ì´ ê°œë…ì€ ì •ë³´ ì´ë¡ ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ë¶„í¬ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” ë° ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤.

### 2. KL Divergenceì™€ Cross Entropyì˜ ê´€ê³„

KL DivergenceëŠ” ë‘ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì„¤ëª…í•˜ëŠ” ê³¼ì •ì—ì„œ, Cross Entropyì™€ Entropyë¥¼ í™œìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤.

- Cross Entropy: $$ğ»(ğ‘,ğ‘)$$ëŠ” ì‹¤ì œ ë¶„í¬$$ğ‘$$ì™€ ì˜ˆì¸¡ ë¶„í¬$$q$$ ê°„ì˜ ì •ë³´ ì°¨ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

- Entropy: $$ğ»(ğ‘)$$ëŠ” ì‹¤ì œ ë¶„í¬ $$p$$ì˜ ì •ë³´ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

**ì¦‰, KL DivergenceëŠ” Cross Entropyì—ì„œ Entropyë¥¼ ëº€ ê°’ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, ì´ëŠ” ì˜ˆì¸¡ ë¶„í¬$$q$$ê°€ ì‹¤ì œ ë¶„í¬$$p$$ì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.**

### 3. KL Divergenceì™€ Cross Entropy ìˆ˜ì‹

KL DivergenceëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤.

$$
D_{KL}(p \| q) = \sum_{i} p(x_i) \log \frac{p(x_i)}{q(x_i)}
$$

ì´ ìˆ˜ì‹ì€ Cross Entropyì™€ Entropyì˜ ì°¨ì´ë¡œë„ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$
D_{KL}(p \| q) = H(p, q) - H(p)
$$

$$H(p,q)$$ëŠ” Cross Entropy, ì‹¤ì œ ë¶„í¬$$p$$ì™€ ì˜ˆì¸¡ ë¶„í¬ $$q$$ ê°„ì˜ ì •ë³´ ì°¨ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

$$H(p)$$ëŠ” Entropy, ì‹¤ì œ ë¶„í¬$$p$$ ìì²´ì˜ ì •ë³´ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

ì´ ìˆ˜ì‹ì€ ë‘ ë¶„í¬ì˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ KL Divergenceê°€ ì»¤ì§€ê³ , ë‘ ë¶„í¬ê°€ ê°™ì„ ë•Œ KL DivergenceëŠ” 0ì´ ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

### 4. Cross Entropyì™€ Entropy ê´€ì ì—ì„œì˜ KL Divergence

Cross Entropy $$H(p,q)$$ëŠ” ì˜ˆì¸¡ ë¶„í¬ $$q$$ê°€ ì‹¤ì œ ë¶„í¬ $$p$$ì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ì„¤ëª…í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

KL DivergenceëŠ” ì´ Cross Entropyì—ì„œ ì‹¤ì œ ë¶„í¬ $$p$$ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” Entropy $$ğ»(ğ‘)$$ë¥¼ ëº€ ê°’ì…ë‹ˆë‹¤. **ë”°ë¼ì„œ KL DivergenceëŠ” ì˜ˆì¸¡ ë¶„í¬ $$ğ‘$$ê°€ ì‹¤ì œ ë¶„í¬ $$ğ‘$$ì— ë¹„í•´ ì–¼ë§ˆë‚˜ ë¹„íš¨ìœ¨ì ì¸ì§€ë¥¼ ì¸¡ì •í•œë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

> ì°¸ê³  ìë£Œ

- [https://www.v7labs.com/blog/cross-entropy-loss-guide](https://www.v7labs.com/blog/cross-entropy-loss-guide)