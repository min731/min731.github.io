---
title: "Entropy / Cross Entropy / KL Divergence"
# author:
#   name: Joung min Lim
#   link: https://github.com/min731
date: 2025-03-02 00:00:00 +0900
# categories: [STEM | 수학/통계, Statistics]
categories: [AI | 딥러닝, Concept]
# categories: [AI ; 논문 리뷰, Attention is all you need]
# categories: [MLOps ; 인프라 개발, Kserve]
# categories: [Life ; 일상 이야기, 와플먹으면서 공부하기]
tags: [DeepLearning, Entropy, Cross Entropy, KL Divergence]
description: "Entropy, Cross Entropy, Kullback–Leibler Divergenc에 대해 알아봅시다."
image: /assets/img/posts/resize/output/entropy.webp # 대표 이미지  가로 세로 비율 약 1.91:1 (예: 1200×628px)
math: true
toc: true
# pin: true
---

**Photo by [https://www.v7labs.com/blog/cross-entropy-loss-guide](https://www.v7labs.com/blog/cross-entropy-loss-guide)**

> ## Entropy

### 1. 정보 이론에서의 Entropy

- 정보의 불확실성이나 혼돈 정도를 측정하는 지표
- 정보 이론에서 엔트로피는 어떤 사건이 발생할 확률에 따라 그 사건의 정보량을 측정합니다. 발생 확률이 낮을수록, 그 사건이 주는 정보량이 큽니다. 반대로, 발생 확률이 높다면 그 사건이 주는 정보량은 적습니다.

- 정보량(Information Content)
$$
I(x) = -\log p(x)
$$

$$
(I(x):사건 x의 정보량,\ p(x):사건 x가 발생할 확률)
$$

### 2. Shannon 엔트로피 (Shannon Entropy)

$$
H(p) = - \sum_{i} p(x_i) \log p(x_i)
$$

$$
(H(p): 확률\ 분포\ p의\ 엔트로피,\ p(x_i):사건 x_i가\ 발생할\ 확률)
$$

### 3. 공정한 동전 던지기의 엔트로피

$$
H = -[0.5 \log_2 0.5 + 0.5 \log_2 0.5] = 1
$$

- 항상 앞면만 나오는 동전의 엔트로피

$$
H = -[1 \log_2 1 + 0 \log_2 0] = 0
$$

### 4. 연속 확률 분포에서의 엔트로피 (Differential Entropy)

$$
H(X) = - \int_{-\infty}^{\infty} p(x) \log p(x) \, dx
$$

> ## Cross Entropy

### 1. Cross Entropy (교차 엔트로피)란?

교차 엔트로피(Cross Entropy)는 실제 확률 분포 $$p$$와 모델이 예측한 확률 분포 $$q$$간의 차이를 측정하는 방법입니다. 주로 분류 문제에서 손실 함수로 많이 사용되며, 모델이 예측한 확률 분포가 실제 분포와 얼마나 다른지를 수치적으로 표현합니다.

교차 엔트로피는 두 확률 분포 간의 평균 정보량을 나타내며, 정답 레이블과 예측 값의 차이를 계산하는 데 사용됩니다. 목표는 예측 분포 $$q$$를 실제 분포
$$p$$와 가깝게 만드는 것입니다.

### 2. 교차 엔트로피 수식

교차 엔트로피는 두 확률 분포 $$𝑝$$와 $$q$$에 대해 다음과 같이 정의됩니다.

- 일반적인 교차 엔트로피 수식
$$
H(p, q) = - \sum_{i} p(x_i) \log q(x_i)
$$

$$
p(x_i): 실제\ 분포에서\ 사건\ 𝑥_𝑖가\ 발생할\ 확률\ (ground\ truth\ 확률)
$$

$$
q(x_i): 예측\ 분포에서\ 사건\ 𝑥_𝑖가\ 발생할\ 확률\ (모델이\ 예측한 확률)
$$

이 수식은 각 사건에 대해 실제 확률과 예측 확률 간의 차이를 로그로 계산한 후, 그 값을 가중합한 것입니다. 교차 엔트로피 값이 작을수록 두 분포 간의 차이가 작고, 값이 클수록 차이가 크다는 의미입니다.

- 이진 분류의 교차 엔트로피 수식
$$
H(p, q) = -[p \log q + (1 - p) \log (1 - q)]
$$

$$
(p:\ 실제\ 레이블\ (0 또는 1),\ q:\ 모델의 예측 확률\  (0 또는 1))
$$

### 3. 교차 엔트로피의 특징

값의 해석: 교차 엔트로피 값이 작을수록 모델이 실제 분포에 가까운 예측을 하고 있다는 의미입니다. 반대로 값이 클수록 모델의 예측이 실제와 많이 다르다는 뜻입니다.

**정확한 예측일 때**: $$𝑞(𝑥_𝑖)$$가 실제 값과 매우 가까우면 $$log𝑞(𝑥_𝑖)$$가 커지고, 교차 엔트로피 값은 작아집니다.

**잘못된 예측일 때**: $$𝑞(𝑥_𝑖)$$가 실제 값과 많이 다를수록 $$log𝑞(𝑥_𝑖)$$는 매우 작아지거나 음수가 되어 교차 엔트로피 값이 커집니다.

> ## KL Divergence

### 1. KL Divergence란?

KL Divergence (Kullback-Leibler Divergence)는 두 확률 분포 간의 차이를 측정하는 방법으로, 실제 분포 $$p$$와 예측 분포 $$q$$가 얼마나 다른지를 나타냅니다. KL Divergence는 비대칭적인 척도이며, **정보 손실 또는 추정의 비효율성을 수치적으로 표현**합니다. 이 개념은 정보 이론에서 중요한 역할을 하며, 분포 사이의 차이를 계산하는 데 자주 사용됩니다.

### 2. KL Divergence와 Cross Entropy의 관계

KL Divergence는 두 확률 분포의 차이를 설명하는 과정에서, Cross Entropy와 Entropy를 활용하여 계산됩니다.

- Cross Entropy: $$𝐻(𝑝,𝑞)$$는 실제 분포$$𝑝$$와 예측 분포$$q$$ 간의 정보 차이를 측정합니다.

- Entropy: $$𝐻(𝑝)$$는 실제 분포 $$p$$의 정보 불확실성을 나타냅니다.

**즉, KL Divergence는 Cross Entropy에서 Entropy를 뺀 값으로 계산되며, 이는 예측 분포$$q$$가 실제 분포$$p$$와 얼마나 다른지를 나타냅니다.**

### 3. KL Divergence와 Cross Entropy 수식

KL Divergence는 다음과 같은 수식으로 정의됩니다.

$$
D_{KL}(p \| q) = \sum_{i} p(x_i) \log \frac{p(x_i)}{q(x_i)}
$$

이 수식은 Cross Entropy와 Entropy의 차이로도 표현할 수 있습니다:

$$
D_{KL}(p \| q) = H(p, q) - H(p)
$$

$$H(p,q)$$는 Cross Entropy, 실제 분포$$p$$와 예측 분포 $$q$$ 간의 정보 차이를 측정합니다.

$$H(p)$$는 Entropy, 실제 분포$$p$$ 자체의 정보 불확실성을 나타냅니다.

이 수식은 두 분포의 차이가 클수록 KL Divergence가 커지고, 두 분포가 같을 때 KL Divergence는 0이 된다는 것을 의미합니다.

### 4. Cross Entropy와 Entropy 관점에서의 KL Divergence

Cross Entropy $$H(p,q)$$는 예측 분포 $$q$$가 실제 분포 $$p$$와 얼마나 다른지를 설명하는데 사용됩니다.

KL Divergence는 이 Cross Entropy에서 실제 분포 $$p$$의 불확실성을 나타내는 Entropy $$𝐻(𝑝)$$를 뺀 값입니다. **따라서 KL Divergence는 예측 분포 $$𝑞$$가 실제 분포 $$𝑝$$에 비해 얼마나 비효율적인지를 측정한다고 할 수 있습니다.**

> 참고 자료

- [https://www.v7labs.com/blog/cross-entropy-loss-guide](https://www.v7labs.com/blog/cross-entropy-loss-guide)